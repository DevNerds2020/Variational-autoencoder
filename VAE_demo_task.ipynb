{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a330b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa88ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data/', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trainset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.PILToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90eaf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data/', train=True, download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9197bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, label in trainloader:\n",
    "    print(X.shape)\n",
    "    print(X.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b5154",
   "metadata": {},
   "source": [
    "![title](auto_encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac112e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.lin1 = nn.Linear(32*7*7, 512)\n",
    "        self.lin2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.conv1(inp)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = out.reshape(inp.shape[0], -1)\n",
    "        out = self.lin1(out)\n",
    "        return self.lin2(self.relu(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7657e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, label in trainloader:\n",
    "    X = X.to(torch.float32)\n",
    "    out = encoder(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(1, 128)\n",
    "        self.lin2 = nn.Linear(128, 512)\n",
    "        self.lin3 = nn.Linear(512, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.lin1(z)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin3(out)\n",
    "        out = out.reshape(z.shape[0], 28, 28)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfabf830",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, label in trainloader:\n",
    "    X = X.to(torch.float32)\n",
    "    print(X.shape)\n",
    "    out = encoder(X)\n",
    "    X_hat = decoder(out).unsqueeze(1)\n",
    "    print(X_hat.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34772b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.encoder(inp)\n",
    "        return self.decoder(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(auto_encoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9daf6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "for e in tqdm(range(epochs)):\n",
    "    for X, label in trainloader:\n",
    "        X = X.to(torch.float32)\n",
    "        X_hat = auto_encoder(X).unsqueeze(1)\n",
    "        loss = criterion(X_hat, X)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(auto_encoder.state_dict(), 'mnist_auto_encoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = Autoencoder()\n",
    "auto_encoder.load_state_dict(torch.load('mnist_auto_encoder.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(cls): # sample image of a particular class from the dataset\n",
    "    for i, data in enumerate(trainset):\n",
    "        if data[1]==cls:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e704c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_image(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd56546",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample[0].squeeze(0).numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = auto_encoder(sample[0].to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35460f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out.squeeze(0).detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3bdb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder.encoder(sample[0].to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10 images for the 10 digits and compare look into the value of z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    sample = sample_image(i)\n",
    "    z = auto_encoder.encoder(sample[0].to(torch.float32))\n",
    "    print(i, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor([[205.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = auto_encoder.decoder(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb286897",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out.squeeze(0).detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6545057",
   "metadata": {},
   "source": [
    "# Variational auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df4e122",
   "metadata": {},
   "source": [
    "![title](vae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6672501",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large D_{KL}\\left[N(\\mu, \\sigma) \\parallel N(0, 1)\\right] = -\\frac{1}{2}\\left(\\log\\sigma^2 + 1 - \\sigma^2 - \\mu^2 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.lin1 = nn.Linear(32*7*7, 512)\n",
    "        self.lin_m = nn.Linear(512, 1) # we will predict \\mu and \\sigma\n",
    "        self.lin_s = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.conv1(inp)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = out.reshape(inp.shape[0], -1)\n",
    "        out = self.lin1(out)\n",
    "        return self.lin_m(self.relu(out)), self.lin_s(self.relu(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variationaautoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder() # same decoder coud be used\n",
    "\n",
    "    def forward(self, X):\n",
    "        m, var = self.encoder(X)\n",
    "        # sample z = \\mu + \\epsilon*\\sigma\n",
    "        z_norm = torch.randn(m.shape[0],1) # generate epsilon\n",
    "        z = m + z_norm*var\n",
    "        return self.decoder(z.unsqueeze(1)), m, var # we need \\mu and \\sigma to compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dafe343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div(m, s):\n",
    "    return 0.5*(m**2 + s**2 - 1 - torch.log(s**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Variationaautoencoder()\n",
    "optimizer = Adam(vae.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6449e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "for e in tqdm(range(epochs)):\n",
    "    running_loss = 0\n",
    "    batches = 0\n",
    "    for X, label in trainloader:\n",
    "        batches+=1\n",
    "        X = X.to(torch.float32)\n",
    "        X_hat, m, s = vae(X)\n",
    "        #print(m,s)\n",
    "        loss = criterion(X_hat.unsqueeze(1), X) + torch.mean(kl_div(m, s))\n",
    "        running_loss+=loss.detach().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Loss at the end of epoch {e}: {running_loss/batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9459af",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'mnist_vae.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f25ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_state_dict(torch.load('mnist_vae.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed727053",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample_image(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7968e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample[0].squeeze(0).numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca109b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = vae(sample[0].to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf311472",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out[0].squeeze(0).detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    sample = sample_image(i)\n",
    "    mu, sigma = vae.encoder(sample[0].to(torch.float32))\n",
    "    print(i, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc66e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c842553",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] =  10,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04437fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 10, num=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "i = 1\n",
    "for m in x:\n",
    "    mu = torch.FloatTensor([[m]])\n",
    "    std = torch.zeros_like(mu)\n",
    "    epsilon = torch.rand_like(std)\n",
    "    z = mu + epsilon*std\n",
    "    out = vae.decoder(z)\n",
    "    plt.subplot(10, 25, i)\n",
    "    plt.imshow(out[0].squeeze(0).detach().numpy(), cmap='gray')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6b05c",
   "metadata": {},
   "source": [
    "# Task: Multiple latent dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308ac40",
   "metadata": {},
   "source": [
    "Consider the same dataset and design a vae consisting of two latent dimensions. So you will have two means and two standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94baa132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. adopt the encoder architecture to predict mu and sigma corresponding to each of the two latent dimensions\n",
    "# 2. adopt the kl-div loss to consider multiple doimensions. Note that it is just the mean of the score across \n",
    "# each individual dimension\n",
    "# 3. the decoder will now take as input a vector of size 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad81bc",
   "metadata": {},
   "source": [
    "Sample images from each class and obtain the corresponding latent vector for each of them and create a scatter plot. Can you identify the clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868a86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "465b47af",
   "metadata": {},
   "source": [
    "Change the number of dimensions to see how it impacts the results.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2297dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
